#!/usr/bin/python3

import multiprocessing
import ntpath
import os
import re
import shlex
import subprocess
import sys
import util

from optparse import OptionParser

(options, args) = util.get_options_and_args()

KNOWN_MAPPINGS = {
  "ash/content": "ash:ash_content_unittests",
  "chrome/browser": "chrome/test:browser_tests",
  "chrome/browser/ui": "chrome/test:interactive_ui_tests",
  "chrome/common/extensions": "chrome/test:unit_tests",
}

def print_rebuild_warning():
  print("If this isn't what you expected, consider re-building unit tests: " + \
      "'crbuild --tests_only'")

def find_all_test_targets():
  global options
  sys.stdout.write("Looking for test targets... ")
  sys.stdout.flush()
  os.chdir(util.get_chromium_src_dir())
  all_targets = subprocess.check_output(shlex.split(
      "gn ls  " + util.get_out_dir())).decode("utf-8").split("\n")
  # Remove the leading "//"
  all_test_targets = sorted([t[2:] for t in all_targets if t.endswith("tests")])
  sys.stdout.write("" + str(len(all_test_targets)) + " targets.")
  print("")
  if options.verbose:
    for t in all_test_targets:
      print("\t" + t)
  return all_test_targets

if len(args) < 1:
  print("I need the name of a test as an argument, e.g. 'base_unittests'.")
  print("For a list of all I know about, try 'crtest list'")
  sys.exit(1)

target = args[0]
list_only = (target == "list")

HOME = os.path.expanduser("~")
TEST_USER_PROFILE_PATH = os.path.join(HOME, "tmp", "test-chromium")
# Even though our target OS isn't necessarily actually Chrome OS, adding this
# gives access to more build targets.
gn_args = util.common_gn_args() + [
  'target_os = "chromeos"',
]

if options.dryrun:
  print("Dry run, not actually running any tests.\n")

all_test_targets = find_all_test_targets()
if list_only:
  print("This is what I know about:\n\t" + "\n\t".join(all_test_targets))
  sys.exit(0)

os.chdir(os.path.join(HOME, "chromium", "src"))

test_method = ""
if "." in target:
  parts = target.split(".", 1)
  target = parts[0]
  test_method = parts[1]
  if options.verbose:
    print("Method name is " + test_method + ", class is " + target)
camel_case = (target != target.lower())
if camel_case:
  if not target.endswith("Test"):
    print("(Note: correcting '" + target + "' into " + target + "Test)")
    target = target + "Test"
  sys.stdout.write("Looking for test files... ")
  sys.stdout.flush()
  all_test_files = sorted(subprocess.check_output(shlex.split(
      'find . -name "*test.cc" -not -path "./third_party/*" -not -path "./out/*" -print'))\
      .decode("utf-8").split("\n"))
  sys.stdout.write("" + str(len(all_test_files)) + " files.\n")
  print("")

  matching_test_files = []
  for f in all_test_files:
    if not os.path.exists(f):
      continue
    data = open(f, "r").read()
    if "class " + target in data or "TEST_F(" + target in data:
      matching_test_files.append(f)

  if len(matching_test_files) == 0:
      print("I didn't find any matching test files.")
  else:
    print("" + target + " is defined in " + str(matching_test_files))

# Now let's try to infere the test target(s).
test_targets = set()
if camel_case and len(matching_test_files) != 0:
  # If the user gave us a camel-cased test name, we just want to find the one
  # target that best matches that. Looking for decreasing sub-paths of the test 
  # file path.
  for matching_test_file in matching_test_files:
    els = matching_test_file.split("/")
    path_length = len(els) - 1
    while (path_length > 1):
      query = "/".join(els[1:path_length])
      if options.verbose:
        print("Trying " + query)
      if query in KNOWN_MAPPINGS:
        test_targets.add(KNOWN_MAPPINGS[query])
        break
      for t in all_test_targets:
        if query + "_unittests" in t or query + "_tests" in query:
          test_targets.add(t)
          break
      path_length -= 1
else:
  for t in all_test_targets:
    # Targets are something like path/to/directory:actual_target_unittests
    parts = t.split("/")
    parts = parts[:-1] + parts[-1].split(":")
    if (("_" in target or "/" in target) and target in t) \
        or target in parts \
        or target + "_unittests" in parts:
      test_targets.add(t)

if len(test_targets) == 0:
  print("Sorry, I didn't find any matching targets. Aborting. ")
  print_rebuild_warning()
  sys.exit(0)

print("\nI'm going to run these test targets:")
for t in test_targets:
  print("\t" + t)
print("")
print_rebuild_warning()

cmd = "gn gen  " + util.get_out_dir() + "  --args='" + " ".join(gn_args) + "'"
util.run(cmd, "Preparing to build...", options)

cmd = ("autoninja -C  " + util.get_out_dir() + "  " + " ".join(test_targets))
status_code = util.run(cmd, "Compiling, this may take a while...", options)
if status_code != 0:
  print("Compilation failed, aborting.")
  sys.exit(status_code)

# TODO: Also use --gtest_filter="*pattern*"
for t in test_targets:
  test_filter = ""
  if camel_case:
    test_filter = "*" + target + "*"
    if test_method:
      test_filter = "*" + target + "." + test_method + "*"
  cmd = "./" + util.get_out_dir() + "/" + \
      (t.split(":")[1] if ":" in t else t) + \
      (' --gtest_filter="' + test_filter + '"' if test_filter else "")
      # Don't use '--use-gpu-in-tests' -- it actually makes tests ~10 times slower
  if options.verbose:
    print(cmd)
  if not options.dryrun:
    os.system(cmd)
